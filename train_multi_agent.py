"""
Multi-Agent Training Script

Train multi-robot coverage system with CTDE and curriculum learning.

Usage:
    python train_multi_agent.py --episodes 1000 --coordination hierarchical
    python train_multi_agent.py --episodes 1000 --agents 4 --parameter_sharing

Key Features:
    - CTDE (Centralized Training, Decentralized Execution)
    - 6-phase curriculum learning
    - Multiple coordination strategies
    - Validation across team sizes and strategies
    - Comprehensive logging and visualization
"""

import argparse
import os
import time
import numpy as np
from datetime import datetime
from typing import Optional

from multi_agent_env import MultiAgentCoverageEnv, CoordinationStrategy
from multi_agent_trainer import MultiAgentTrainer
from multi_agent_config import ma_config
from multi_agent_curriculum import (
    get_multi_agent_curriculum_phase,
    get_multi_agent_epsilon,
    get_multi_agent_map_type
)
from config import config
from communication import get_communication_protocol
from agent_occupancy import AgentOccupancyComputer


def create_directories():
    """Create necessary directories for results."""
    os.makedirs(ma_config.VIS_DIR, exist_ok=True)
    os.makedirs(ma_config.CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(ma_config.METRICS_DIR, exist_ok=True)
    print(f"✓ Created directories:")
    print(f"  {ma_config.VIS_DIR}")
    print(f"  {ma_config.CHECKPOINT_DIR}")
    print(f"  {ma_config.METRICS_DIR}")


def log_episode(episode: int, episode_info: dict, trainer: MultiAgentTrainer):
    """Log episode information."""
    team_reward = episode_info['team_reward']
    coverage = episode_info['team_coverage']
    length = episode_info['episode_length']
    collisions = episode_info['collisions']
    agent_collisions = episode_info['agent_collisions']
    epsilon = episode_info['epsilon']

    print(f"Ep {episode:4d} | "
          f"Reward: {team_reward:7.1f} | "
          f"Coverage: {coverage*100:5.1f}% | "
          f"Length: {length:3d} | "
          f"Collisions: {collisions:2d} ({agent_collisions} agent) | "
          f"ε: {epsilon:.3f}")


def validate_and_save(
    episode: int,
    trainer: MultiAgentTrainer,
    env: MultiAgentCoverageEnv,
    experiment_name: str,
    comm_manager=None,
    occupancy_computer=None
):
    """Run validation and save checkpoint."""
    print(f"\n{'='*70}")
    print(f"VALIDATION @ Episode {episode}")
    print(f"{'='*70}")

    # Validate on current configuration
    val_results = trainer.validate(
        env,
        num_episodes=ma_config.VALIDATION_EPISODES,
        map_types=ma_config.VALIDATION_MAP_TYPES,
        comm_manager=comm_manager,
        occupancy_computer=occupancy_computer
    )

    print(f"\nValidation Results:")
    print(f"  Mean Coverage: {val_results['mean_coverage']*100:.1f}% "
          f"(±{val_results['std_coverage']*100:.1f}%)")
    print(f"  Mean Team Reward: {val_results['mean_team_reward']:.1f}")
    print(f"  Mean Length: {val_results['mean_length']:.0f}")
    print(f"  Mean Collisions: {val_results['mean_collisions']:.1f}")
    print(f"  Mean Coordination Score: {val_results['mean_coordination_score']:.1f}/100 "
          f"(±{val_results['std_coordination_score']:.1f})")

    print(f"\nPer-Map Coverage:")
    for map_type, coverage in val_results['per_map_coverage'].items():
        print(f"  {map_type:12s}: {coverage*100:.1f}%")

    # Save checkpoint
    checkpoint_path = os.path.join(
        ma_config.CHECKPOINT_DIR,
        f"{experiment_name}_ep{episode}.pth"
    )
    trainer.save(checkpoint_path)
    print(f"\n✓ Saved checkpoint: {checkpoint_path}")

    print(f"{'='*70}\n")

    return val_results


def train_multi_agent(
    num_agents: int = 4,
    total_episodes: int = 1000,
    coordination: CoordinationStrategy = CoordinationStrategy.INDEPENDENT,
    parameter_sharing: bool = False,  # FIXED: No parameter sharing for independent agents
    shared_replay: bool = True,
    use_curriculum: bool = True,
    use_qmix: bool = False,
    comm_protocol: str = 'position',  # FIXED: Default to position-based communication
    experiment_name: Optional[str] = None,
    resume_from: Optional[str] = None
):
    """
    Main multi-agent training loop with 6-channel input (always enabled).

    Args:
        num_agents: Number of agents
        total_episodes: Total training episodes
        coordination: Coordination strategy
        parameter_sharing: Use parameter sharing (default False for independent agents)
        shared_replay: Use shared replay memory
        use_curriculum: Use curriculum learning
        use_qmix: Use QMIX for centralized training with decentralized execution
        comm_protocol: Communication protocol ('none', 'position', 'full_state')
        experiment_name: Experiment name (auto-generated if None)
        resume_from: Path to checkpoint to resume from
    """
    # Create experiment name
    if experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comm_suffix = f"_{comm_protocol}" if comm_protocol != 'none' else ""
        qmix_suffix = "_qmix" if use_qmix else ""
        experiment_name = f"ma{num_agents}_{coordination.value}_6ch{comm_suffix}{qmix_suffix}_{timestamp}"

    print(f"\n{'='*70}")
    print(f"MULTI-AGENT COVERAGE TRAINING")
    print(f"{'='*70}")
    print(f"Experiment: {experiment_name}")
    print(f"Agents: {num_agents}")
    print(f"Input Channels: 6 (with predictive agent occupancy)")
    print(f"Communication: {comm_protocol}")
    print(f"QMIX (CTDE): {'ENABLED' if use_qmix else 'DISABLED'}")
    print(f"Coordination: {coordination.value}")
    print(f"Parameter Sharing: {parameter_sharing}")
    print(f"Shared Replay: {shared_replay}")
    print(f"Curriculum: {use_curriculum}")
    print(f"Environment: {'PROBABILISTIC' if config.USE_PROBABILISTIC_ENV else 'BINARY'} (USE_PROBABILISTIC_ENV={config.USE_PROBABILISTIC_ENV})")
    if config.USE_PROBABILISTIC_ENV:
        print(f"  Sigmoid: k={config.PROBABILISTIC_COVERAGE_STEEPNESS}, r0={config.PROBABILISTIC_COVERAGE_MIDPOINT}, threshold={config.COVERAGE_THRESHOLD}")
    print(f"Total Episodes: {total_episodes}")
    print(f"{'='*70}\n")

    # Create directories
    create_directories()

    # Initialize environment
    env = MultiAgentCoverageEnv(
        num_agents=num_agents,
        grid_size=ma_config.GRID_SIZE,
        sensor_range=ma_config.SENSOR_RANGE,
        communication_range=ma_config.COMMUNICATION_RANGE,
        coordination=coordination,
        team_reward_weight=ma_config.TEAM_REWARD_WEIGHT,
        collision_penalty=ma_config.AGENT_COLLISION_PENALTY
    )

    # Initialize trainer with QMIX support - ALWAYS 6 channels
    trainer = MultiAgentTrainer(
        num_agents=num_agents,
        grid_size=ma_config.GRID_SIZE,
        coordination=coordination,
        parameter_sharing=parameter_sharing,
        shared_replay=shared_replay,
        input_channels=6,
        use_qmix=use_qmix
    )

    # Load pre-trained single-agent checkpoint if provided
    if resume_from is not None:
        print(f"\n{'='*70}")
        print(f"LOADING PRE-TRAINED CHECKPOINT")
        print(f"{'='*70}")
        print(f"Checkpoint: {resume_from}")
        
        if parameter_sharing:
            # Load into shared agent
            trainer.agents[0].load(resume_from)
            print(f"✓ Loaded checkpoint into shared agent network")
            print(f"  All {num_agents} agents will use this pre-trained network")
        else:
            # Load into all independent agents
            for i, agent in enumerate(trainer.agents):
                agent.load(resume_from)
                print(f"✓ Loaded checkpoint into agent {i}")
            print(f"  All {num_agents} agents initialized with same pre-trained weights")
        
        print(f"{'='*70}\n")

    # Initialize communication protocol
    comm_manager = get_communication_protocol(
        protocol_name=comm_protocol,
        num_agents=num_agents,
        grid_size=ma_config.GRID_SIZE,
        comm_range=ma_config.COMMUNICATION_RANGE
    )
    print(f"✓ Communication protocol: {comm_protocol}")

    # Initialize agent occupancy computer with predictive trajectories (ALWAYS enabled)
    occupancy_computer = AgentOccupancyComputer(
        grid_size=ma_config.GRID_SIZE,
        base_sigma=0.5,
        max_velocity=1.0,
        time_decay_rate=0.1,
        prediction_horizon=5,  # Project 5 steps into future
        use_predictive=True    # Enable trajectory-based prediction
    )
    print(f"✓ Agent occupancy computation enabled (PREDICTIVE mode)")
    print(f"  - Prediction horizon: 5 timesteps")
    print(f"  - Creates Gaussian probability maps along predicted trajectories")
    print(f"  - Enables proactive collision avoidance and path coordination")

    print(f"✓ Environment and trainer initialized\n")

    # Training metrics
    all_validation_results = []
    start_time = time.time()

    # Current curriculum phase
    current_phase = None

    # Training loop
    for episode in range(total_episodes):

        # Update curriculum phase
        if use_curriculum:
            phase = get_multi_agent_curriculum_phase(episode)

            if phase != current_phase:
                current_phase = phase
                print(f"\n{'='*70}")
                print(f"CURRICULUM PHASE CHANGE @ Episode {episode}")
                print(f"{'='*70}")
                print(f"Phase: {phase.description}")
                print(f"Episodes: {phase.episode_start}-{phase.episode_end}")
                print(f"Coverage Target: {phase.coverage_target*100:.0f}%")
                print(f"Overlap Target: ≤{phase.overlap_target*100:.0f}%")
                print(f"Epsilon: {phase.epsilon_start:.3f} → {phase.epsilon_end:.3f}")
                print(f"{'='*70}\n")

            # Get map type from curriculum
            map_type = get_multi_agent_map_type(episode)

            # Update epsilon based on curriculum
            epsilon = get_multi_agent_epsilon(episode)
            trainer.epsilon = epsilon
            trainer.set_epsilon(epsilon)

        else:
            # No curriculum - use default settings
            map_type = None
            trainer.decay_epsilon(decay_rate=0.995)

        # Train episode
        episode_info = trainer.train_episode(
            env, 
            map_type=map_type,
            comm_manager=comm_manager,
            occupancy_computer=occupancy_computer
        )

        # Log progress
        if episode % ma_config.LOG_FREQ == 0:
            # Enhanced logging with coordination metrics
            coord_score = episode_info.get('coordination_score', 0.0)
            coord_metrics = episode_info.get('coordination_metrics', None)

            # Derive overlap and efficiency (prefer coord_metrics object if present)
            if coord_metrics is not None:
                overlap_pct = coord_metrics.overlap_ratio * 100
                efficiency_pct = coord_metrics.exploration_efficiency * 100
            else:
                overlap_val = episode_info.get('overlap', None)
                efficiency_val = episode_info.get('efficiency', None)
                overlap_pct = (overlap_val * 100) if overlap_val is not None else float('nan')
                efficiency_pct = (efficiency_val * 100) if efficiency_val is not None else float('nan')

            print(f"Ep {episode:4d} | "
                  f"Cov: {episode_info['team_coverage']*100:5.1f}% | "
                  f"Overlap: {overlap_pct:5.1f}% | "
                  f"Coord: {coord_score:5.1f}/100 | "
                  f"Rew: {episode_info['team_reward']:7.1f} | "
                  f"Len: {episode_info['episode_length']:3d} | "
                  f"Eff: {efficiency_pct:5.1f}% | "
                  f"ε: {trainer.epsilon:.3f}")
            
            # DEBUG: Print coverage map statistics
            # Show Agent 0's local coverage (not shared world_state coverage_map)
            observations = env.get_observations()
            if observations:
                cov_map = observations[0]['robot_state'].coverage_history
                print(f"  [DEBUG] Steps={episode_info['episode_length']} | "
                      f"Agent0 coverage: min={np.min(cov_map):.3f}, max={np.max(cov_map):.3f}, "
                      f"mean={np.mean(cov_map):.3f}, cells>0.85={np.sum(cov_map >= 0.85)}")
                # Compact coordination summary (print every LOG_FREQ)
                # Prefer values from coord_metrics if present, else fall back to episode_info
                if coord_metrics is not None:
                    overlap_pct = coord_metrics.overlap_ratio * 100
                    efficiency_pct = coord_metrics.exploration_efficiency * 100
                    balance_val = coord_metrics.load_balance_ratio
                    collisions_short = coord_metrics.agent_collisions + coord_metrics.obstacle_collisions
                else:
                    overlap_val = episode_info.get('overlap', None)
                    efficiency_val = episode_info.get('efficiency', None)
                    overlap_pct = (overlap_val * 100) if overlap_val is not None else float('nan')
                    efficiency_pct = (efficiency_val * 100) if efficiency_val is not None else float('nan')
                    balance_val = episode_info.get('balance', float('nan'))
                    collisions_short = episode_info.get('collisions', 0)

                print(f"  Overlap: {overlap_pct:5.1f}% | Efficiency: {efficiency_pct:5.1f}% | "
                      f"Balance: {balance_val:.2f} | Collisions: {collisions_short}")
            
            if coord_metrics and episode % (ma_config.LOG_FREQ * 5) == 0:
                # Print detailed coordination breakdown every 5*LOG_FREQ episodes
                print(f"  Overlap: {coord_metrics.overlap_ratio*100:.1f}% | "
                      f"Efficiency: {coord_metrics.exploration_efficiency*100:.1f}% | "
                      f"Balance: {coord_metrics.load_balance_ratio:.2f} | "
                      f"Collisions: {coord_metrics.agent_collisions + coord_metrics.obstacle_collisions}")

        # Validation
        if (episode + 1) % ma_config.VALIDATION_FREQ == 0:
            val_results = validate_and_save(
                episode + 1,
                trainer,
                env,
                experiment_name,
                comm_manager=comm_manager,
                occupancy_computer=occupancy_computer
            )
            all_validation_results.append({
                'episode': episode + 1,
                'results': val_results
            })

        # Periodic stats
        if (episode + 1) % ma_config.PLOT_FREQ == 0:
            stats = trainer.get_training_stats(window=100)
            print(f"\n{'='*70}")
            print(f"TRAINING STATS @ Episode {episode + 1}")
            print(f"{'='*70}")
            print(f"  Mean Team Reward (100 ep): {stats['mean_team_reward']:.1f}")
            print(f"  Mean Coverage (100 ep): {stats['mean_coverage']*100:.1f}%")
            print(f"  Mean Length (100 ep): {stats['mean_length']:.0f}")
            print(f"  Mean Collisions (100 ep): {stats['mean_collisions']:.1f}")
            
            if 'mean_coordination_score' in stats:
                print(f"  Mean Coordination (100 ep): {stats['mean_coordination_score']:.1f}/100")
            
            print(f"  Epsilon: {stats['epsilon']:.3f}")

            if 'mean_loss' in stats:
                print(f"  Mean Loss (100 ep): {stats['mean_loss']:.4f}")

            elapsed = time.time() - start_time
            eps_per_sec = (episode + 1) / elapsed
            remaining = (total_episodes - episode - 1) / eps_per_sec

            print(f"\n  Elapsed: {elapsed/3600:.1f}h")
            print(f"  Speed: {eps_per_sec:.2f} ep/s")
            print(f"  Remaining: {remaining/3600:.1f}h")
            print(f"{'='*70}\n")

    # Final validation
    print(f"\n{'='*70}")
    print(f"FINAL VALIDATION")
    print(f"{'='*70}\n")

    final_val_results = validate_and_save(
        total_episodes,
        trainer,
        env,
        experiment_name,
        comm_manager=comm_manager,
        occupancy_computer=occupancy_computer
    )

    # Save final model
    final_path = os.path.join(
        ma_config.CHECKPOINT_DIR,
        f"{experiment_name}_FINAL.pth"
    )
    trainer.save(final_path)
    print(f"✓ Saved final model: {final_path}")

    # Training summary
    total_time = time.time() - start_time
    print(f"\n{'='*70}")
    print(f"TRAINING COMPLETE")
    print(f"{'='*70}")
    print(f"  Total Episodes: {total_episodes}")
    print(f"  Total Time: {total_time/3600:.2f} hours")
    print(f"  Average Speed: {total_episodes/total_time:.2f} ep/s")
    print(f"\n  Final Coverage: {final_val_results['mean_coverage']*100:.1f}% "
          f"(±{final_val_results['std_coverage']*100:.1f}%)")
    print(f"  Final Team Reward: {final_val_results['mean_team_reward']:.1f}")
    print(f"{'='*70}\n")

    return trainer, all_validation_results


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Train multi-agent coverage system"
    )

    # Training parameters
    parser.add_argument(
        '--episodes',
        type=int,
        default=1000,
        help='Total training episodes'
    )

    parser.add_argument(
        '--agents',
        type=int,
        default=4,
        help='Number of agents [2-8]'
    )

    parser.add_argument(
        '--coordination',
        type=str,
        default='independent',
        choices=['independent', 'hierarchical'],
        help='Coordination strategy'
    )

    parser.add_argument(
        '--no-parameter-sharing',
        action='store_true',
        help='Disable parameter sharing (independent networks)'
    )

    parser.add_argument(
        '--no-shared-replay',
        action='store_true',
        help='Disable shared replay memory (separate buffers)'
    )

    parser.add_argument(
        '--no-curriculum',
        action='store_true',
        help='Disable curriculum learning'
    )

    parser.add_argument(
        '--use-qmix',
        action='store_true',
        help='Use QMIX for centralized training with joint Q-value learning'
    )

    parser.add_argument(
        '--comm-protocol',
        type=str,
        default='position',
        choices=['none', 'position'],
        help='Communication protocol (default: position). Options: none (no comm), position (position/velocity broadcast for predictive occupancy).'
    )

    parser.add_argument(
        '--experiment-name',
        type=str,
        default=None,
        help='Experiment name (auto-generated if not provided)'
    )

    parser.add_argument(
        '--resume-from',
        type=str,
        default=None,
        help='Path to single-agent checkpoint (fcn_final.pt) to initialize agent networks'
    )

    parser.add_argument(
        '--probabilistic',
        action='store_true',
        help='Use probabilistic environment (sigmoid coverage) instead of binary'
    )

    args = parser.parse_args()

    # Apply probabilistic environment setting if specified
    if args.probabilistic:
        config.USE_PROBABILISTIC_ENV = True

    # Parse coordination strategy
    coordination_map = {
        'independent': CoordinationStrategy.INDEPENDENT,
        'hierarchical': CoordinationStrategy.HIERARCHICAL
    }
    coordination = coordination_map[args.coordination]

    # Train - ALWAYS uses 6 channels with predictive occupancy
    train_multi_agent(
        num_agents=args.agents,
        total_episodes=args.episodes,
        coordination=coordination,
        parameter_sharing=not args.no_parameter_sharing,
        shared_replay=not args.no_shared_replay,
        use_curriculum=not args.no_curriculum,
        use_qmix=args.use_qmix,
        comm_protocol=args.comm_protocol,
        experiment_name=args.experiment_name,
        resume_from=args.resume_from
    )


if __name__ == "__main__":
    main()
